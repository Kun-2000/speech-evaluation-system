"""
èªéŸ³è½‰éŒ„è©•ä¼°ç³»çµ±æ ¸å¿ƒæ¨¡çµ„
æ•´åˆ STTã€LLMã€å ±å‘Šç”ŸæˆåŠŸèƒ½
"""

import logging
import time
from datetime import datetime
from pathlib import Path
from typing import Union
import uuid

from .stt import transcribe_audio
from .llm import compare_text_accuracy
from ..config import EVALUATION_CONFIG, get_evaluation_thresholds

logger = logging.getLogger(__name__)


class EvaluationResult:
    """è©•ä¼°çµæœæ•¸æ“šé¡"""

    def __init__(self):
        """åˆå§‹åŒ–è©•ä¼°çµæœç‰©ä»¶"""
        self.evaluation_id = str(uuid.uuid4())
        self.timestamp = datetime.now().isoformat()
        self.audio_file = ""
        self.reference_text = ""
        self.transcription = {}
        self.comparison = {}
        self.evaluation_metrics = {}
        self.processing_time = 0.0
        self.success = False
        self.error_message = None

    def to_dict(self) -> dict:
        """å°‡çµæœç‰©ä»¶è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
        return {
            "evaluation_id": self.evaluation_id,
            "timestamp": self.timestamp,
            "audio_file": self.audio_file,
            "reference_text": self.reference_text,
            "transcription": self.transcription,
            "comparison": self.comparison,
            "evaluation_metrics": self.evaluation_metrics,
            "processing_time": self.processing_time,
            "success": self.success,
            "error_message": self.error_message,
        }


class EvaluationService:
    """è©•ä¼°æœå‹™æ ¸å¿ƒé¡"""

    def __init__(self):
        """åˆå§‹åŒ–è©•ä¼°æœå‹™"""
        self.config = EVALUATION_CONFIG
        logger.info("âœ… è©•ä¼°æœå‹™åˆå§‹åŒ–æˆåŠŸ")

    def evaluate_single_file(
        self, audio_file_path: Union[str, Path], reference_text: str
    ) -> EvaluationResult:
        """è©•ä¼°å–®å€‹éŸ³é »æª”æ¡ˆ"""
        result = EvaluationResult()
        start_time = time.time()

        try:
            result.audio_file = str(audio_file_path)
            result.reference_text = reference_text

            logger.info("ğŸ”„ é–‹å§‹è©•ä¼°æª”æ¡ˆ: %s", Path(audio_file_path).name)

            transcript, confidence = transcribe_audio(audio_file_path)

            result.transcription = {
                "file_path": str(audio_file_path),
                "file_name": Path(audio_file_path).name,
                "transcript": transcript,
                "confidence": confidence,
                "success": True,
                "error": None,
            }

            logger.info(
                "âœ… èªéŸ³è½‰éŒ„å®Œæˆ: %s",
                transcript[:30] + "..." if len(transcript) > 30 else transcript,
            )

            comparison = compare_text_accuracy(transcript, reference_text)
            comparison["success"] = True
            result.comparison = comparison

            logger.info(
                "âœ… æ–‡å­—æ¯”å°å®Œæˆ - æº–ç¢ºç‡: %.1f%%", comparison.get("accuracy_score", 0)
            )

            result.evaluation_metrics = self._calculate_evaluation_metrics(
                comparison, confidence
            )

            result.success = True

        except (RuntimeError, ValueError) as e:
            logger.error("âŒ æª”æ¡ˆè©•ä¼°å¤±æ•—: %s", e, exc_info=True)
            result.error_message = str(e)
            result.success = False

            if not result.transcription:
                result.transcription = {
                    "file_path": str(audio_file_path),
                    "file_name": Path(audio_file_path).name,
                    "transcript": None,
                    "confidence": 0.0,
                    "success": False,
                    "error": str(e),
                }

            if not result.comparison:
                result.comparison = {
                    "summary": "è©•ä¼°å¤±æ•—",
                    "accuracy_score": 0.0,
                    "semantic_similarity": 0.0,
                    "error_analysis": {
                        "substitutions": 0,
                        "deletions": 0,
                        "insertions": 0,
                        "total_errors": 0,
                    },
                    "key_differences": [],
                    "suggestions": ["æª¢æŸ¥éŸ³é »æª”æ¡ˆ", "ç¢ºèªç¶²è·¯é€£ç·š"],
                    "reasoning": f"è©•ä¼°éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
                    "success": False,
                    "error": str(e),
                }

            result.evaluation_metrics = {
                "accuracy_score": 0.0,
                "semantic_similarity": 0.0,
                "accuracy_level": "è©•ä¼°å¤±æ•—",
                "confidence": 0.0,
                "processing_status": "failed",
            }

        finally:
            result.processing_time = round(time.time() - start_time, 2)
            logger.info("ğŸ“Š æª”æ¡ˆè©•ä¼°å®Œæˆï¼Œç”¨æ™‚ %.2f ç§’", result.processing_time)

        return result

    def _calculate_evaluation_metrics(
        self, comparison: dict, confidence: float
    ) -> dict:
        """è¨ˆç®—è©•ä¼°æŒ‡æ¨™"""
        accuracy_score = comparison.get("accuracy_score", 0)
        semantic_similarity = comparison.get("semantic_similarity", 0)

        thresholds = get_evaluation_thresholds()
        if accuracy_score >= thresholds["excellent"]:
            accuracy_level = "å„ªç§€"
        elif accuracy_score >= thresholds["good"]:
            accuracy_level = "è‰¯å¥½"
        elif accuracy_score >= thresholds["fair"]:
            accuracy_level = "æ™®é€š"
        else:
            accuracy_level = "éœ€è¦æ”¹é€²"

        return {
            "accuracy_score": accuracy_score,
            "semantic_similarity": semantic_similarity,
            "accuracy_level": accuracy_level,
            "confidence": confidence,
            "processing_status": "completed",
        }


# pylint: disable=invalid-name
evaluation_service = None
try:
    evaluation_service = EvaluationService()
    logger.info("âœ… è©•ä¼°å…¨åŸŸæœå‹™åˆå§‹åŒ–æˆåŠŸ")

except (RuntimeError, ValueError) as e:
    logger.warning("âŒ è©•ä¼°æœå‹™åˆå§‹åŒ–å¤±æ•—: %s", e)


def evaluate_single_file(
    audio_file_path: Union[str, Path], reference_text: str
) -> EvaluationResult:
    """
    è©•ä¼°å–®å€‹éŸ³é »æª”æ¡ˆçš„è½‰éŒ„å“è³ª
    """
    if not evaluation_service:
        raise RuntimeError("è©•ä¼°æœå‹™ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥ç³»çµ±é…ç½®")

    return evaluation_service.evaluate_single_file(audio_file_path, reference_text)
