# 語音轉錄品質評估系統 (Speech-to-Text Evaluation System)

本專案為一套基於 Web 的語音轉錄 (Speech-to-Text) 品質評估系統。系統旨在提供自動化、智慧化的評估流程，透過大型語言模型 (LLM) 對 STT 的轉錄結果進行深度分析，並生成量化指標與質化建議。

## 目錄

- [1. 專案目標](#1-專案目標)
- [2. 應用場景](#2-應用場景)
- [3. 核心功能](#3-核心功能)
- [4. 關鍵技術選型與分析](#4-關鍵技術選型與分析)
- [5. 成本考量與比較](#5-成本考量與比較)
- [6. 系統部署與啟動](#6-系統部署與啟動)
- [7. 系統架構](#7-系統架構)
- [8. 模組結構詳解](#8-模組結構詳解)

## 1. 專案目標

傳統的 STT 評估 (如字詞錯誤率 WER) 通常僅限於字面比對，難以衡量語意層面的準確性。本專案旨在解決此問題，引入大型語言模型作為評估核心，以達成以下目標：

- **自動化評估**：建立一個無需人工干預的自動化評估流程。
- **深度分析**：除了字面準確率，同時評估轉錄稿的語意相似度。
- **量化報告**：提供包含準確率、錯誤類型統計（替換、刪除、插入）的量化數據。
- **質化回饋**：利用 LLM 的生成能力，提供具體的改進建議。

## 2. 應用場景

本系統可應用於多種需要進行語音轉文字品質驗證的商業或研究情境：

### STT 服務供應商評估

在導入新的語音轉文字服務前，可利用本系統對市面上不同的模型（如 OpenAI Whisper, Google Speech-to-Text 等）進行客觀的量化評估與橫向比較，以選擇最符合業務需求的方案。

### 模型迭代與回歸測試

當內部 STT 模型進行版本更新，或針對特定垂直領域（如醫療、法律、金融）進行微調 (Fine-tuning) 後，可利用此系統驗證新模型的效能指標，確保模型更新帶來正面效益且未在其他方面產生效能衰退 (Regression)。

### 語音數據標註品管

對於需要大量語音標註資料的 AI 應用，本系統可反向用於評估人工聽打逐字稿的準確性與一致性，作為數據品質控管的一環。

### 學術研究與教育

可作為自然語言處理 (NLP) 或語音辨識領域的教學與研究工具，讓研究者能直觀地分析與比較不同 STT 演算法的錯誤類型與語意保真度。

## 3. 核心功能

- **雙模式音訊輸入**：支援本地音訊檔案上傳（支援 MP3, WAV, M4A 等多種格式）與瀏覽器即時麥克風錄音。

- **高品質語音轉錄**：整合 OpenAI Whisper 模型進行語音轉文字。

- **採用大型語言模型 (LLM) 進行智慧評估**：利用 OpenAI GPT-4o-mini 模型，對比轉錄稿與參考文本，進行語意與語法層面的綜合分析。

- **互動式視覺化報告**：評估結果以動態 Web 介面呈現，包含準確率、語意相似度、錯誤統計圖表及 AI 生成的改進建議。

- **非同步處理介面**：前端介面能即時反饋後端處理進度（轉錄、分析、完成），確保流暢的使用者體驗。

- **模組化後端架構**：後端採用 Flask 框架，將各核心服務（STT, LLM, Evaluation）高度解耦，提升系統的可維護性與擴充性。

## 4. 關鍵技術選型與分析

本系統的技術選型旨在平衡評估的深度、成本效益與實現的複雜度。

### 4.1 評估核心：LLM vs. 傳統指標

本專案選擇使用大型語言模型作為評估核心，而非僅依賴傳統的字詞錯誤率 (WER) 指標。

| 評估方法         | 優點                                                                                                            | 缺點                                                                                  | 本專案選用理由                                                                   |
| ---------------- | --------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| **LLM 智慧評估** | 1. 能理解語意，可處理同義詞、語序變換等複雜情況<br>2. 能分析語法流暢度與上下文連貫性<br>3. 可生成質化的改進建議 | 1. 評估成本較高 (API 費用)<br>2. 處理速度相對較慢<br>3. 結果可能存在輕微不確定性      | 追求超越字面比對的深度分析，提供更具商業洞察力的評估報告，這是本專案的核心價值。 |
| **傳統 WER/CER** | 1. 計算速度極快，可即時產出<br>2. 無額外 API 成本，運算開銷極低<br>3. 結果完全確定，可 100% 重現                | 1. 完全無法理解語意<br>2. 對標點、資字等非核心錯誤極其敏感<br>3. 無法提供任何改進建議 | 僅能量化字面層級的差異，無法反映真實的溝通有效性，不符合本專案的深度評估目標。   |

### 4.2 語音轉錄服務：OpenAI Whisper

本專案預設採用 OpenAI Whisper 作為 STT 引擎，主要基於以下考量：

- **通用性與準確性**：對多種主流語言（包含國語、英語）及不同口音的基礎辨識能力已達頂尖水準，無需額外訓練即可獲得高品質的轉錄結果。

- **成本效益**：相較於其他提供同等級準確率的商業服務，Whisper API 的定價策略極具市場競爭力。

- **易於整合**：OpenAI 提供穩定且文件清晰的 RESTful API，簡化了系統整合的複雜度。

## 5. 成本考量與比較

本系統的運營成本主要來自對 OpenAI API 的兩部分呼叫，成本完全透明且按用量計費。

1. **STT 成本**：來自 `Whisper` API，按處理的音訊分鐘數計費。

2. **LLM 成本**：來自 `GPT-4o-mini` API，按輸入與輸出的 Token 總數量計費。

### 成本估算範例

以下提供一個具體的計算案例，以助於理解單次評估的成本結構。

**情境假設**：

- 評估一個長度為 **5 分鐘**的音訊檔案。
- 該音訊轉錄後的文本長度約為 800 字（約 1,200 個 OpenAI Token）。
- 本系統的 LLM 分析提示 (Prompt) 及回傳的 JSON 報告，合計約消耗 1,500 個 Token。

### 預估成本分析（費率參考 2025 年 Q3）

#### STT (Whisper) 費用

- `5 分鐘 × $0.006 / 分鐘 = $0.03 USD`

#### LLM (GPT-4o-mini) 費用

- 輸入：(1200 + Prompt) Token，輸出：(JSON) Token
- 假設總共處理 `3,000` 個 Token (1,200 + 1,500 + 300)
- 輸入：`$0.15 / 1M tokens` 輸出：`$0.60 / 1M tokens`
- 費用約為 `(2700 × 0.15 + 300 × 0.60) / 1,000,000 ≈ $0.000585 USD`

#### 單次評估總成本

- `$0.03 (STT) + $0.000585 (LLM) ≈ $0.0306 USD`
- **約等於 TWD 0.90 元（以匯率 1:29.44 計算）**

> **註**：以上為估算值，實際成本將因文本具體長度與 API 費率變動而異。

## 6. 系統部署與啟動

### 前置需求

- Python 3.8 或更高版本
- `git` 版本控制工具

### 部署步驟

#### 步驟 1: 取得原始碼

```bash
git clone https://github.com/Kun-2000/speech-evaluation-system.git
cd speech-evaluation-system
```

#### 步驟 2: 建立並啟用 Python 虛擬環境

為確保依賴套件隔離，建議使用虛擬環境。

```bash
# macOS / Linux
python3 -m venv venv && source venv/bin/activate

# Windows
python -m venv venv && .\venv\Scripts\activate
```

#### 步驟 3: 安裝依賴套件

專案所需之套件皆定義於 `requirements.txt`。

```bash
pip install -r requirements.txt
```

> **註記**：PyAudio 套件在部分作業系統可能需要額外的系統級函式庫（如 `portaudio`）。

#### 步驟 4: 設定環境變數

在專案根目錄建立 `.env` 檔案。此檔常用於存放 API 金鑰等敏感資訊。

```env
# .env

# [必要] OpenAI API 金鑰
OPENAI_API_KEY="sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# [可選] 覆蓋預設模型或其他設定
# OPENAI_STT_MODEL=whisper-1
# OPENAI_LLM_MODEL=gpt-4o-mini
# MAX_FILE_SIZE=100
```

#### 步驟 5: 啟動應用程式

```bash
python run.py
```

應用程式成功啟動後，即可透過瀏覽器訪問 `http://127.0.0.1:5000`。

## 7. 系統架構

本系統採用前後端分離的設計模式，後端遵循服務導向架構 (Service-Oriented Architecture)。

### 運作流程 (Workflow)

1. **前端使用者介面 (Frontend UI - `index.html`)**：負責使用者互動、狀態管理與 API 請求。使用者提交音訊與參考文本後，前端將請求發送至後端 API。

2. **API 路由層 (API Gateway - `app.py`)**：Flask 應用作為 API 入口，負責接收請求、驗證輸入（檔案格式、大小等），並將業務邏輯分派至對應的服務層。

3. **評估服務層 (Evaluation Service - `evaluation.py`)**：作為核心協調者 (Orchestrator)，此模組負責調度完整的評估管線 (pipeline)。

4. **數據回傳與渲染**：`app.py` 將 `evaluation_service` 產出的 `EvaluationResult` 物件序列化為 JSON 格式回傳給前端。前端 JavaScript 接收到數據後，動態渲染至結果顯示區。

## 8. 模組結構詳解

- **`run.py`**：應用程式主入口。負責執行啟動前的環境配置驗證、清理暫存檔案，並啟動 Flask Web 伺服器。

- **`src/speech_analyzer/`**：核心原始碼目錄。

  - **`app.py`**：Flask 應用實例。定義所有 API 端點、請求處理、裝飾器及全局錯誤處理機制。
  - **`config.py`**：設定管理模組。負責從 `.env` 檔案載入環境變數，並將其組織成可供全域使用的設定物件。
  - **`services/`**：核心商業邏輯層。
    - **`stt.py`**：封裝 STT 相關功能。`OpenAISTTClient` 負責與 Whisper API 互動；`AudioRecorder` 則透過 `PyAudio` 與 `threading` 實現非阻塞的即時錄音。
    - **`llm.py`**：封裝 LLM 相關功能。`LLMService` 的核心職責是建構 Few-Shot Prompt 並解析 LLM 回傳的 JSON 結果，確保分析的穩定性與一致性。
    - **`evaluation.py`**：業務流程協調模組。整合 `stt_service` 與 `llm_service`，執行完整的評估流程並產出最終報告物件。

- **`templates/index.html`**：應用程式的前端介面。單一 HTML 檔案內含負責結構的 HTML、負責樣式的 CSS，以及負責所有互動邏輯的 JavaScript。

- **`requirements.txt`**：定義專案運作所需的所有 Python 依賴套件及其版本。
